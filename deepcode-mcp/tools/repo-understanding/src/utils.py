"""
å·¥å…·å‡½æ•°
"""
import os
import aiofiles
from typing import Optional, Dict, Any
from pathlib import Path
import chromadb.utils.embedding_functions as embedding_functions

def detect_language(file_path: str) -> Optional[str]:
    """æ£€æµ‹æ–‡ä»¶çš„ç¼–ç¨‹è¯­è¨€"""
    from .analyzer import CodeAnalyzer
    
    ext = os.path.splitext(file_path)[1].lower()
    return CodeAnalyzer.SUPPORTED_EXTENSIONS.get(ext)

async def read_file_safe(file_path: str, encoding: str = 'utf-8') -> Optional[str]:
    """å®‰å…¨åœ°è¯»å–æ–‡ä»¶å†…å®¹"""
    try:
        async with aiofiles.open(file_path, 'r', encoding=encoding) as f:
            return await f.read()
    except UnicodeDecodeError:
        # å°è¯•å…¶ä»–ç¼–ç 
        try:
            async with aiofiles.open(file_path, 'r', encoding='latin-1') as f:
                return await f.read()
        except:
            return None
    except Exception:
        return None

def get_embedding_function():
    """èŽ·å–åµŒå…¥å‡½æ•°"""
    # ä¼˜å…ˆä½¿ç”¨OpenAIï¼Œå¦‚æžœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤çš„
    api_key = os.getenv("OPENAI_API_KEY")
    if api_key:
        return embedding_functions.OpenAIEmbeddingFunction(
            api_key=api_key,
            model_name="text-embedding-3-small"
        )
    else:
        # ä½¿ç”¨é»˜è®¤çš„å¥å­è½¬æ¢å™¨
        return embedding_functions.SentenceTransformerEmbeddingFunction(
            model_name="all-MiniLM-L6-v2"
        )

def format_file_tree(structure: Dict[str, Any], 
                    indent: str = "", 
                    is_last: bool = True,
                    current_depth: int = 0,
                    max_depth: int = 3) -> str:
    """æ ¼å¼åŒ–æ–‡ä»¶æ ‘ç»“æž„"""
    if current_depth >= max_depth:
        return ""
    
    output = ""
    
    # èŽ·å–æ–‡ä»¶å’Œç›®å½•
    files = structure.get('files', [])
    directories = structure.get('directories', {})
    
    # æŽ’åº
    files.sort()
    dir_items = sorted(directories.items())
    
    # æ˜¾ç¤ºæ–‡ä»¶
    for i, file in enumerate(files):
        is_last_item = (i == len(files) - 1) and not directories
        prefix = "â””â”€â”€ " if is_last_item else "â”œâ”€â”€ "
        output += f"{indent}{prefix}ðŸ“„ {file}\n"
    
    # æ˜¾ç¤ºç›®å½•
    for i, (dir_name, dir_content) in enumerate(dir_items):
        is_last_item = (i == len(dir_items) - 1)
        prefix = "â””â”€â”€ " if is_last_item else "â”œâ”€â”€ "
        output += f"{indent}{prefix}ðŸ“ {dir_name}/\n"
        
        # é€’å½’æ˜¾ç¤ºå­ç›®å½•
        new_indent = indent + ("    " if is_last_item else "â”‚   ")
        output += format_file_tree(
            dir_content, 
            new_indent, 
            is_last_item,
            current_depth + 1,
            max_depth
        )
    
    return output

def truncate_content(content: str, max_lines: int = 20) -> str:
    """æˆªæ–­å†…å®¹åˆ°æŒ‡å®šè¡Œæ•°"""
    lines = content.split('\n')
    if len(lines) <= max_lines:
        return content
    
    truncated = '\n'.join(lines[:max_lines])
    truncated += f"\n... ({len(lines) - max_lines} more lines)"
    return truncated

def calculate_metrics(repo_info: Dict[str, Any]) -> Dict[str, Any]:
    """è®¡ç®—ä»“åº“æŒ‡æ ‡"""
    metrics = {
        'total_files': repo_info.get('total_files', 0),
        'total_lines': repo_info.get('total_lines', 0),
        'languages': repo_info.get('languages', {}),
        'primary_language': None,
        'language_diversity': 0
    }
    
    if metrics['languages']:
        # ä¸»è¦è¯­è¨€
        metrics['primary_language'] = max(
            metrics['languages'].items(), 
            key=lambda x: x[1]
        )[0]
        
        # è¯­è¨€å¤šæ ·æ€§ï¼ˆä½¿ç”¨é¦™å†œç†µï¼‰
        total_lines = metrics['total_lines']
        if total_lines > 0:
            import math
            entropy = 0
            for lines in metrics['languages'].values():
                p = lines / total_lines
                if p > 0:
                    entropy -= p * math.log2(p)
            metrics['language_diversity'] = entropy
    
    return metrics