#!/usr/bin/env python3
"""
Code Implementation MCP Server - ä»£ç å®ç°MCPæœåŠ¡å™¨

è¿™ä¸ªMCPæœåŠ¡å™¨æä¾›äº†è®ºæ–‡ä»£ç å¤ç°æ‰€éœ€çš„æ ¸å¿ƒåŠŸèƒ½ï¼š
1. æ–‡ä»¶è¯»å†™æ“ä½œ (File read/write operations)
2. ä»£ç æ‰§è¡Œå’Œæµ‹è¯• (Code execution and testing)
3. ä»£ç æœç´¢å’Œåˆ†æ (Code search and analysis)
4. è¿­ä»£å¼æ”¹è¿›æ”¯æŒ (Iterative improvement support)

This MCP server provides core functions needed for paper code reproduction:
1. File read/write operations
2. Code execution and testing
3. Code search and analysis
4. Iterative improvement support

ä½¿ç”¨æ–¹æ³• / Usage:
python tools/code_implementation_server.py
"""

import os
import subprocess
import json
import sys
import io
from pathlib import Path
import re
from typing import Dict, Any
import tempfile
import shutil
import logging
from datetime import datetime

# è®¾ç½®æ ‡å‡†è¾“å‡ºç¼–ç ä¸ºUTF-8
if sys.stdout.encoding != "utf-8":
    try:
        if hasattr(sys.stdout, "reconfigure"):
            sys.stdout.reconfigure(encoding="utf-8")
            sys.stderr.reconfigure(encoding="utf-8")
        else:
            sys.stdout = io.TextIOWrapper(sys.stdout.detach(), encoding="utf-8")
            sys.stderr = io.TextIOWrapper(sys.stderr.detach(), encoding="utf-8")
    except Exception as e:
        print(f"Warning: Could not set UTF-8 encoding: {e}")

# å¯¼å…¥MCPç›¸å…³æ¨¡å—
from mcp.server.fastmcp import FastMCP

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# åˆ›å»ºFastMCPæœåŠ¡å™¨å®ä¾‹
mcp = FastMCP("code-implementation-server")

# å…¨å±€å˜é‡ï¼šå·¥ä½œç©ºé—´ç›®å½•å’Œæ“ä½œå†å²
WORKSPACE_DIR = None
OPERATION_HISTORY = []
CURRENT_FILES = {}


def initialize_workspace(workspace_dir: str = None):
    """
    åˆå§‹åŒ–å·¥ä½œç©ºé—´
    
    é»˜è®¤æƒ…å†µä¸‹ï¼Œå·¥ä½œç©ºé—´å°†é€šè¿‡ set_workspace å·¥å…·ç”±å·¥ä½œæµè®¾ç½®ä¸º:
    {plan_file_parent}/generate_code
    
    Args:
        workspace_dir: å¯é€‰çš„å·¥ä½œç©ºé—´ç›®å½•è·¯å¾„
    """
    global WORKSPACE_DIR
    if workspace_dir is None:
        # é»˜è®¤ä½¿ç”¨å½“å‰ç›®å½•ä¸‹çš„generate_codeç›®å½•ï¼Œä½†ä¸ç«‹å³åˆ›å»º
        # è¿™ä¸ªé»˜è®¤å€¼å°†è¢«å·¥ä½œæµé€šè¿‡ set_workspace å·¥å…·è¦†ç›–
        WORKSPACE_DIR = Path.cwd() / "generate_code"
        # logger.info(f"å·¥ä½œç©ºé—´åˆå§‹åŒ– (é»˜è®¤å€¼ï¼Œå°†è¢«å·¥ä½œæµè¦†ç›–): {WORKSPACE_DIR}")
        # logger.info("æ³¨æ„: å®é™…å·¥ä½œç©ºé—´å°†ç”±å·¥ä½œæµé€šè¿‡ set_workspace å·¥å…·è®¾ç½®ä¸º {plan_file_parent}/generate_code")
    else:
        WORKSPACE_DIR = Path(workspace_dir).resolve()
        # åªæœ‰æ˜ç¡®æŒ‡å®šç›®å½•æ—¶æ‰åˆ›å»º
        WORKSPACE_DIR.mkdir(parents=True, exist_ok=True)
        logger.info(f"å·¥ä½œç©ºé—´åˆå§‹åŒ–: {WORKSPACE_DIR}")


def ensure_workspace_exists():
    """ç¡®ä¿å·¥ä½œç©ºé—´ç›®å½•å­˜åœ¨"""
    global WORKSPACE_DIR
    if WORKSPACE_DIR is None:
        initialize_workspace()
    
    # åˆ›å»ºå·¥ä½œç©ºé—´ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    if not WORKSPACE_DIR.exists():
        WORKSPACE_DIR.mkdir(parents=True, exist_ok=True)
        logger.info(f"å·¥ä½œç©ºé—´ç›®å½•å·²åˆ›å»º: {WORKSPACE_DIR}")


def validate_path(path: str) -> Path:
    """éªŒè¯è·¯å¾„æ˜¯å¦åœ¨å·¥ä½œç©ºé—´å†…"""
    if WORKSPACE_DIR is None:
        initialize_workspace()

    full_path = (WORKSPACE_DIR / path).resolve()
    if not str(full_path).startswith(str(WORKSPACE_DIR)):
        raise ValueError(f"è·¯å¾„ {path} è¶…å‡ºå·¥ä½œç©ºé—´èŒƒå›´")
    return full_path


def log_operation(action: str, details: Dict[str, Any]):
    """è®°å½•æ“ä½œå†å²"""
    OPERATION_HISTORY.append(
        {"timestamp": datetime.now().isoformat(), "action": action, "details": details}
    )


# ==================== æ–‡ä»¶æ“ä½œå·¥å…· ====================


@mcp.tool()
async def read_file(
    file_path: str, start_line: int = None, end_line: int = None
) -> str:
    """
    è¯»å–æ–‡ä»¶å†…å®¹ï¼Œæ”¯æŒæŒ‡å®šè¡Œå·èŒƒå›´

    Args:
        file_path: æ–‡ä»¶è·¯å¾„ï¼Œç›¸å¯¹äºå·¥ä½œç©ºé—´
        start_line: èµ·å§‹è¡Œå·ï¼ˆä»1å¼€å§‹ï¼Œå¯é€‰ï¼‰
        end_line: ç»“æŸè¡Œå·ï¼ˆä»1å¼€å§‹ï¼Œå¯é€‰ï¼‰

    Returns:
        æ–‡ä»¶å†…å®¹æˆ–é”™è¯¯ä¿¡æ¯çš„JSONå­—ç¬¦ä¸²
    """
    try:
        full_path = validate_path(file_path)

        if not full_path.exists():
            result = {"status": "error", "message": f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"}
            log_operation(
                "read_file_error", {"file_path": file_path, "error": "file_not_found"}
            )
            return json.dumps(result, ensure_ascii=False, indent=2)

        with open(full_path, "r", encoding="utf-8") as f:
            lines = f.readlines()

        # å¤„ç†è¡Œå·èŒƒå›´
        if start_line is not None or end_line is not None:
            start_idx = (start_line - 1) if start_line else 0
            end_idx = end_line if end_line else len(lines)
            lines = lines[start_idx:end_idx]

        content = "".join(lines)

        result = {
            "status": "success",
            "content": content,
            "file_path": file_path,
            "total_lines": len(lines),
            "size_bytes": len(content.encode("utf-8")),
        }

        log_operation(
            "read_file",
            {
                "file_path": file_path,
                "start_line": start_line,
                "end_line": end_line,
                "lines_read": len(lines),
            },
        )

        return json.dumps(result, ensure_ascii=False, indent=2)

    except Exception as e:
        result = {
            "status": "error",
            "message": f"è¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}",
            "file_path": file_path,
        }
        log_operation("read_file_error", {"file_path": file_path, "error": str(e)})
        return json.dumps(result, ensure_ascii=False, indent=2)


@mcp.tool()
async def write_file(
    file_path: str, content: str, create_dirs: bool = True, create_backup: bool = False
) -> str:
    """
    å†™å…¥å†…å®¹åˆ°æ–‡ä»¶

    Args:
        file_path: æ–‡ä»¶è·¯å¾„ï¼Œç›¸å¯¹äºå·¥ä½œç©ºé—´
        content: è¦å†™å…¥çš„æ–‡ä»¶å†…å®¹
        create_dirs: å¦‚æœç›®å½•ä¸å­˜åœ¨æ˜¯å¦åˆ›å»º
        create_backup: å¦‚æœæ–‡ä»¶å·²å­˜åœ¨æ˜¯å¦åˆ›å»ºå¤‡ä»½æ–‡ä»¶

    Returns:
        æ“ä½œç»“æœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        full_path = validate_path(file_path)

        # åˆ›å»ºç›®å½•ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if create_dirs:
            full_path.parent.mkdir(parents=True, exist_ok=True)

        # å¤‡ä»½ç°æœ‰æ–‡ä»¶ï¼ˆä»…åœ¨æ˜ç¡®è¦æ±‚æ—¶ï¼‰
        backup_created = False
        if full_path.exists() and create_backup:
            backup_path = full_path.with_suffix(full_path.suffix + ".backup")
            shutil.copy2(full_path, backup_path)
            backup_created = True

        # å†™å…¥æ–‡ä»¶
        with open(full_path, "w", encoding="utf-8") as f:
            f.write(content)

        # æ›´æ–°å½“å‰æ–‡ä»¶è®°å½•
        CURRENT_FILES[file_path] = {
            "last_modified": datetime.now().isoformat(),
            "size_bytes": len(content.encode("utf-8")),
            "lines": len(content.split("\n")),
        }

        result = {
            "status": "success",
            "message": f"æ–‡ä»¶å†™å…¥æˆåŠŸ: {file_path}",
            "file_path": file_path,
            "size_bytes": len(content.encode("utf-8")),
            "lines_written": len(content.split("\n")),
            "backup_created": backup_created,
        }

        log_operation(
            "write_file",
            {
                "file_path": file_path,
                "size_bytes": len(content.encode("utf-8")),
                "lines": len(content.split("\n")),
                "backup_created": backup_created,
            },
        )

        return json.dumps(result, ensure_ascii=False, indent=2)

    except Exception as e:
        result = {
            "status": "error",
            "message": f"å†™å…¥æ–‡ä»¶å¤±è´¥: {str(e)}",
            "file_path": file_path,
        }
        log_operation("write_file_error", {"file_path": file_path, "error": str(e)})
        return json.dumps(result, ensure_ascii=False, indent=2)


# ==================== ä»£ç æ‰§è¡Œå·¥å…· ====================


@mcp.tool()
async def execute_python(code: str, timeout: int = 30) -> str:
    """
    æ‰§è¡ŒPythonä»£ç å¹¶è¿”å›è¾“å‡º

    Args:
        code: è¦æ‰§è¡Œçš„Pythonä»£ç 
        timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

    Returns:
        æ‰§è¡Œç»“æœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(
            mode="w", suffix=".py", delete=False, encoding="utf-8"
        ) as f:
            f.write(code)
            temp_file = f.name

        try:
            # ç¡®ä¿å·¥ä½œç©ºé—´ç›®å½•å­˜åœ¨
            ensure_workspace_exists()
            
            # æ‰§è¡ŒPythonä»£ç 
            result = subprocess.run(
                [sys.executable, temp_file],
                cwd=WORKSPACE_DIR,
                capture_output=True,
                text=True,
                timeout=timeout,
                encoding="utf-8",
            )

            execution_result = {
                "status": "success" if result.returncode == 0 else "error",
                "return_code": result.returncode,
                "stdout": result.stdout,
                "stderr": result.stderr,
                "timeout": timeout,
            }

            if result.returncode != 0:
                execution_result["message"] = "Pythonä»£ç æ‰§è¡Œå¤±è´¥"
            else:
                execution_result["message"] = "Pythonä»£ç æ‰§è¡ŒæˆåŠŸ"

            log_operation(
                "execute_python",
                {
                    "return_code": result.returncode,
                    "stdout_length": len(result.stdout),
                    "stderr_length": len(result.stderr),
                },
            )

            return json.dumps(execution_result, ensure_ascii=False, indent=2)

        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.unlink(temp_file)

    except subprocess.TimeoutExpired:
        result = {
            "status": "error",
            "message": f"Pythonä»£ç æ‰§è¡Œè¶…æ—¶ ({timeout}ç§’)",
            "timeout": timeout,
        }
        log_operation("execute_python_timeout", {"timeout": timeout})
        return json.dumps(result, ensure_ascii=False, indent=2)

    except Exception as e:
        result = {"status": "error", "message": f"Pythonä»£ç æ‰§è¡Œå¤±è´¥: {str(e)}"}
        log_operation("execute_python_error", {"error": str(e)})
        return json.dumps(result, ensure_ascii=False, indent=2)


@mcp.tool()
async def execute_bash(command: str, timeout: int = 30) -> str:
    """
    æ‰§è¡Œbashå‘½ä»¤

    Args:
        command: è¦æ‰§è¡Œçš„bashå‘½ä»¤
        timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

    Returns:
        æ‰§è¡Œç»“æœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        # å®‰å…¨æ£€æŸ¥ï¼šç¦æ­¢å±é™©å‘½ä»¤
        dangerous_commands = ["rm -rf", "sudo", "chmod 777", "mkfs", "dd if="]
        if any(dangerous in command.lower() for dangerous in dangerous_commands):
            result = {"status": "error", "message": f"ç¦æ­¢æ‰§è¡Œå±é™©å‘½ä»¤: {command}"}
            log_operation(
                "execute_bash_blocked",
                {"command": command, "reason": "dangerous_command"},
            )
            return json.dumps(result, ensure_ascii=False, indent=2)

        # ç¡®ä¿å·¥ä½œç©ºé—´ç›®å½•å­˜åœ¨
        ensure_workspace_exists()
        
        # æ‰§è¡Œå‘½ä»¤
        result = subprocess.run(
            command,
            shell=True,
            cwd=WORKSPACE_DIR,
            capture_output=True,
            text=True,
            timeout=timeout,
            encoding="utf-8",
        )

        execution_result = {
            "status": "success" if result.returncode == 0 else "error",
            "return_code": result.returncode,
            "stdout": result.stdout,
            "stderr": result.stderr,
            "command": command,
            "timeout": timeout,
        }

        if result.returncode != 0:
            execution_result["message"] = "Bashå‘½ä»¤æ‰§è¡Œå¤±è´¥"
        else:
            execution_result["message"] = "Bashå‘½ä»¤æ‰§è¡ŒæˆåŠŸ"

        log_operation(
            "execute_bash",
            {
                "command": command,
                "return_code": result.returncode,
                "stdout_length": len(result.stdout),
                "stderr_length": len(result.stderr),
            },
        )

        return json.dumps(execution_result, ensure_ascii=False, indent=2)

    except subprocess.TimeoutExpired:
        result = {
            "status": "error",
            "message": f"Bashå‘½ä»¤æ‰§è¡Œè¶…æ—¶ ({timeout}ç§’)",
            "command": command,
            "timeout": timeout,
        }
        log_operation("execute_bash_timeout", {"command": command, "timeout": timeout})
        return json.dumps(result, ensure_ascii=False, indent=2)

    except Exception as e:
        result = {
            "status": "error",
            "message": f"Bashå‘½ä»¤æ‰§è¡Œå¤±è´¥: {str(e)}",
            "command": command,
        }
        log_operation("execute_bash_error", {"command": command, "error": str(e)})
        return json.dumps(result, ensure_ascii=False, indent=2)


@mcp.tool()
async def read_code_mem(file_path: str) -> str:
    """
    Check if file summary exists in implement_code_summary.md
    
    Args:
        file_path: File path to check for summary information in implement_code_summary.md
        
    Returns:
        Summary information if available
    """
    try:
        if not file_path:
            result = {
                "status": "error",
                "message": "file_path parameter is required"
            }
            log_operation("read_code_mem_error", {"error": "missing_file_path"})
            return json.dumps(result, ensure_ascii=False, indent=2)
        
        # Ensure workspace exists
        ensure_workspace_exists()
        
        # Look for implement_code_summary.md in the workspace
        current_path = Path(WORKSPACE_DIR)
        summary_file_path = current_path.parent / "implement_code_summary.md"
        
        if not summary_file_path.exists():
            result = {
                "status": "no_summary",
                "file_path": file_path,
                "message": f"No summary file found.",
                # "recommendation": f"read_file(file_path='{file_path}')"
            }
            log_operation("read_code_mem", {"file_path": file_path, "status": "no_summary_file"})
            return json.dumps(result, ensure_ascii=False, indent=2)
        
        # Read the summary file
        with open(summary_file_path, 'r', encoding='utf-8') as f:
            summary_content = f.read()
        
        if not summary_content.strip():
            result = {
                "status": "no_summary",
                "file_path": file_path,
                "message": f"Summary file is empty.",
                # "recommendation": f"read_file(file_path='{file_path}')"
            }
            log_operation("read_code_mem", {"file_path": file_path, "status": "empty_summary"})
            return json.dumps(result, ensure_ascii=False, indent=2)
        
        # Extract file-specific section from summary
        file_section = _extract_file_section_from_summary(summary_content, file_path)
        
        if file_section:
            result = {
                "status": "summary_found",
                "file_path": file_path,
                "summary_content": file_section,
                "message": f"Summary information found for {file_path} in implement_code_summary.md"
            }
            log_operation("read_code_mem", {"file_path": file_path, "status": "summary_found", "section_length": len(file_section)})
            return json.dumps(result, ensure_ascii=False, indent=2)
        else:
            result = {
                "status": "no_summary",
                "file_path": file_path,
                "message": f"No summary found for {file_path} in implement_code_summary.md",
                # "recommendation": f"Use read_file tool to read the actual file: read_file(file_path='{file_path}')"
            }
            log_operation("read_code_mem", {"file_path": file_path, "status": "no_match"})
            return json.dumps(result, ensure_ascii=False, indent=2)
            
    except Exception as e:
        result = {
            "status": "error",
            "message": f"Failed to check code memory: {str(e)}",
            "file_path": file_path,
            # "recommendation": "Use read_file tool instead"
        }
        log_operation("read_code_mem_error", {"file_path": file_path, "error": str(e)})
        return json.dumps(result, ensure_ascii=False, indent=2)


def _extract_file_section_from_summary(summary_content: str, target_file_path: str) -> str:
    """
    Extract the specific section for a file from the summary content
    
    Args:
        summary_content: Full summary content
        target_file_path: Path of the target file
        
    Returns:
        File-specific section or None if not found
    """
    import re
    
    # Normalize the target path for comparison
    normalized_target = _normalize_file_path(target_file_path)
    
    # Pattern to match implementation sections with separator lines
    section_pattern = r'={80}\s*\n## IMPLEMENTATION File ([^;]+); ROUND \d+\s*\n={80}(.*?)(?=\n={80}|\Z)'
    
    matches = re.findall(section_pattern, summary_content, re.DOTALL)
    
    for file_path_in_summary, section_content in matches:
        file_path_in_summary = file_path_in_summary.strip()
        section_content = section_content.strip()
        
        # Normalize the path from summary for comparison
        normalized_summary_path = _normalize_file_path(file_path_in_summary)
        
        # Check if paths match using multiple strategies
        if _paths_match(normalized_target, normalized_summary_path, target_file_path, file_path_in_summary):
            # Return the complete section with proper formatting
            file_section = f"""================================================================================
## IMPLEMENTATION File {file_path_in_summary}; ROUND [X]
================================================================================

{section_content}

---
*Extracted from implement_code_summary.md*"""
            return file_section
    
    # If no section-based match, try alternative parsing method
    return _extract_file_section_alternative(summary_content, target_file_path)


def _normalize_file_path(file_path: str) -> str:
    """Normalize file path for comparison"""
    # Remove leading/trailing slashes and convert to lowercase
    normalized = file_path.strip('/').lower()
    # Replace backslashes with forward slashes
    normalized = normalized.replace('\\', '/')
    
    # Remove common prefixes to make matching more flexible
    common_prefixes = ['rice/', 'src/', './rice/', './src/', './']
    for prefix in common_prefixes:
        if normalized.startswith(prefix):
            normalized = normalized[len(prefix):]
            break
    
    return normalized


def _paths_match(normalized_target: str, normalized_summary: str, original_target: str, original_summary: str) -> bool:
    """Check if two file paths match using multiple strategies"""
    
    # Strategy 1: Exact normalized match
    if normalized_target == normalized_summary:
        return True
    
    # Strategy 2: Basename match (filename only)
    target_basename = os.path.basename(original_target)
    summary_basename = os.path.basename(original_summary)
    if target_basename == summary_basename and len(target_basename) > 4:
        return True
    
    # Strategy 3: Suffix match (remove common prefixes and compare)
    target_suffix = _remove_common_prefixes(normalized_target)
    summary_suffix = _remove_common_prefixes(normalized_summary)
    if target_suffix == summary_suffix:
        return True
    
    # Strategy 4: Ends with match
    if normalized_target.endswith(normalized_summary) or normalized_summary.endswith(normalized_target):
        return True
    
    # Strategy 5: Contains match for longer paths
    if len(normalized_target) > 10 and normalized_target in normalized_summary:
        return True
    if len(normalized_summary) > 10 and normalized_summary in normalized_target:
        return True
    
    return False


def _remove_common_prefixes(file_path: str) -> str:
    """Remove common prefixes from file path"""
    prefixes_to_remove = ['rice/', 'src/', 'core/', './']
    path = file_path
    
    for prefix in prefixes_to_remove:
        if path.startswith(prefix):
            path = path[len(prefix):]
    
    return path


def _extract_file_section_alternative(summary_content: str, target_file_path: str) -> str:
    """Alternative method to extract file section using simpler pattern matching"""
    
    # Get the basename for fallback matching
    target_basename = os.path.basename(target_file_path)
    
    # Split by separator lines to get individual sections
    sections = summary_content.split('=' * 80)
    
    for i, section in enumerate(sections):
        if '## IMPLEMENTATION File' in section:
            # Extract the file path from the header
            lines = section.strip().split('\n')
            for line in lines:
                if '## IMPLEMENTATION File' in line:
                    # Extract file path between "File " and "; ROUND"
                    try:
                        file_part = line.split('File ')[1].split('; ROUND')[0].strip()
                        
                        # Check if this matches our target
                        if (_normalize_file_path(target_file_path) == _normalize_file_path(file_part) or
                            target_basename == os.path.basename(file_part) or
                            target_file_path in file_part or
                            file_part.endswith(target_file_path)):
                            
                            # Get the next section which contains the content
                            if i + 1 < len(sections):
                                content_section = sections[i + 1].strip()
                                return f"""================================================================================
## IMPLEMENTATION File {file_part}
================================================================================

{content_section}

---
*Extracted from implement_code_summary.md using alternative method*"""
                    except (IndexError, AttributeError):
                        continue
    
    return None


# ==================== ä»£ç æœç´¢å·¥å…· ====================


@mcp.tool()
async def search_code(
    pattern: str, 
    file_pattern: str = "*.json", 
    use_regex: bool = False,
    search_directory: str = None
) -> str:
    """
    åœ¨ä»£ç æ–‡ä»¶ä¸­æœç´¢æ¨¡å¼

    Args:
        pattern: æœç´¢æ¨¡å¼
        file_pattern: æ–‡ä»¶æ¨¡å¼ï¼ˆå¦‚ '*.py'ï¼‰
        use_regex: æ˜¯å¦ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼
        search_directory: æŒ‡å®šæœç´¢ç›®å½•ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™ä½¿ç”¨WORKSPACE_DIRï¼‰

    Returns:
        æœç´¢ç»“æœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        # ç¡®å®šæœç´¢ç›®å½•
        if search_directory:
            # å¦‚æœæŒ‡å®šäº†æœç´¢ç›®å½•ï¼Œä½¿ç”¨æŒ‡å®šçš„ç›®å½•
            if os.path.isabs(search_directory):
                search_path = Path(search_directory)
            else:
                # ç›¸å¯¹è·¯å¾„ï¼Œç›¸å¯¹äºå½“å‰å·¥ä½œç›®å½•
                search_path = Path.cwd() / search_directory
        else:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šæœç´¢ç›®å½•ï¼Œä½¿ç”¨é»˜è®¤çš„WORKSPACE_DIR
            ensure_workspace_exists()
            search_path = WORKSPACE_DIR
        
        # æ£€æŸ¥æœç´¢ç›®å½•æ˜¯å¦å­˜åœ¨
        if not search_path.exists():
            result = {
                "status": "error",
                "message": f"æœç´¢ç›®å½•ä¸å­˜åœ¨: {search_path}",
                "pattern": pattern,
            }
            return json.dumps(result, ensure_ascii=False, indent=2)

        import glob

        # è·å–åŒ¹é…çš„æ–‡ä»¶
        file_paths = glob.glob(str(search_path / "**" / file_pattern), recursive=True)

        matches = []
        total_files_searched = 0

        for file_path in file_paths:
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    lines = f.readlines()

                total_files_searched += 1
                relative_path = os.path.relpath(file_path, search_path)

                for line_num, line in enumerate(lines, 1):
                    if use_regex:
                        if re.search(pattern, line):
                            matches.append(
                                {
                                    "file": relative_path,
                                    "line_number": line_num,
                                    "line_content": line.strip(),
                                    "match_type": "regex",
                                }
                            )
                    else:
                        if pattern.lower() in line.lower():
                            matches.append(
                                {
                                    "file": relative_path,
                                    "line_number": line_num,
                                    "line_content": line.strip(),
                                    "match_type": "substring",
                                }
                            )

            except Exception as e:
                logger.warning(f"æœç´¢æ–‡ä»¶æ—¶å‡ºé”™ {file_path}: {e}")
                continue

        result = {
            "status": "success",
            "pattern": pattern,
            "file_pattern": file_pattern,
            "use_regex": use_regex,
            "search_directory": str(search_path),
            "total_matches": len(matches),
            "total_files_searched": total_files_searched,
            "matches": matches[:50],  # é™åˆ¶è¿”å›å‰50ä¸ªåŒ¹é…
        }

        if len(matches) > 50:
            result["note"] = f"æ˜¾ç¤ºå‰50ä¸ªåŒ¹é…ï¼Œæ€»å…±æ‰¾åˆ°{len(matches)}ä¸ªåŒ¹é…"

        log_operation(
            "search_code",
            {
                "pattern": pattern,
                "file_pattern": file_pattern,
                "use_regex": use_regex,
                "search_directory": str(search_path),
                "total_matches": len(matches),
                "files_searched": total_files_searched,
            },
        )

        return json.dumps(result, ensure_ascii=False, indent=2)

    except Exception as e:
        result = {
            "status": "error",
            "message": f"ä»£ç æœç´¢å¤±è´¥: {str(e)}",
            "pattern": pattern,
        }
        log_operation("search_code_error", {"pattern": pattern, "error": str(e)})
        return json.dumps(result, ensure_ascii=False, indent=2)


# ==================== æ–‡ä»¶ç»“æ„å·¥å…· ====================


@mcp.tool()
async def get_file_structure(directory: str = ".", max_depth: int = 5) -> str:
    """
    è·å–ç›®å½•çš„æ–‡ä»¶ç»“æ„

    Args:
        directory: ç›®å½•è·¯å¾„ï¼Œç›¸å¯¹äºå·¥ä½œç©ºé—´
        max_depth: æœ€å¤§éå†æ·±åº¦

    Returns:
        æ–‡ä»¶ç»“æ„çš„JSONå­—ç¬¦ä¸²
    """
    try:
        ensure_workspace_exists()

        if directory == ".":
            target_dir = WORKSPACE_DIR
        else:
            target_dir = validate_path(directory)

        if not target_dir.exists():
            result = {"status": "error", "message": f"ç›®å½•ä¸å­˜åœ¨: {directory}"}
            return json.dumps(result, ensure_ascii=False, indent=2)

        def scan_directory(path: Path, current_depth: int = 0) -> Dict[str, Any]:
            """é€’å½’æ‰«æç›®å½•"""
            if current_depth >= max_depth:
                return {"type": "directory", "name": path.name, "truncated": True}

            items = []
            try:
                for item in sorted(path.iterdir()):
                    relative_path = os.path.relpath(item, WORKSPACE_DIR)

                    if item.is_file():
                        file_info = {
                            "type": "file",
                            "name": item.name,
                            "path": relative_path,
                            "size_bytes": item.stat().st_size,
                            "extension": item.suffix,
                        }
                        items.append(file_info)
                    elif item.is_dir() and not item.name.startswith("."):
                        dir_info = scan_directory(item, current_depth + 1)
                        dir_info["path"] = relative_path
                        items.append(dir_info)
            except PermissionError:
                pass

            return {
                "type": "directory",
                "name": path.name,
                "items": items,
                "item_count": len(items),
            }

        structure = scan_directory(target_dir)

        # ç»Ÿè®¡ä¿¡æ¯
        def count_items(node):
            if node["type"] == "file":
                return {"files": 1, "directories": 0}
            else:
                counts = {"files": 0, "directories": 1}
                for item in node.get("items", []):
                    item_counts = count_items(item)
                    counts["files"] += item_counts["files"]
                    counts["directories"] += item_counts["directories"]
                return counts

        counts = count_items(structure)

        result = {
            "status": "success",
            "directory": directory,
            "max_depth": max_depth,
            "structure": structure,
            "summary": {
                "total_files": counts["files"],
                "total_directories": counts["directories"] - 1,  # å‡å»æ ¹ç›®å½•
            },
        }

        log_operation(
            "get_file_structure",
            {
                "directory": directory,
                "max_depth": max_depth,
                "total_files": counts["files"],
                "total_directories": counts["directories"] - 1,
            },
        )

        return json.dumps(result, ensure_ascii=False, indent=2)

    except Exception as e:
        result = {
            "status": "error",
            "message": f"è·å–æ–‡ä»¶ç»“æ„å¤±è´¥: {str(e)}",
            "directory": directory,
        }
        log_operation(
            "get_file_structure_error", {"directory": directory, "error": str(e)}
        )
        return json.dumps(result, ensure_ascii=False, indent=2)


# ==================== å·¥ä½œç©ºé—´ç®¡ç†å·¥å…· ====================


@mcp.tool()
async def set_workspace(workspace_path: str) -> str:
    """
    è®¾ç½®å·¥ä½œç©ºé—´ç›®å½•
    
    ç”±å·¥ä½œæµè°ƒç”¨ä»¥å°†å·¥ä½œç©ºé—´è®¾ç½®ä¸º: {plan_file_parent}/generate_code
    è¿™ç¡®ä¿æ‰€æœ‰æ–‡ä»¶æ“ä½œéƒ½ç›¸å¯¹äºæ­£ç¡®çš„é¡¹ç›®ç›®å½•æ‰§è¡Œ

    Args:
        workspace_path: å·¥ä½œç©ºé—´è·¯å¾„ (é€šå¸¸æ˜¯ {plan_file_parent}/generate_code)

    Returns:
        æ“ä½œç»“æœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        global WORKSPACE_DIR
        new_workspace = Path(workspace_path).resolve()

        # åˆ›å»ºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        new_workspace.mkdir(parents=True, exist_ok=True)

        old_workspace = WORKSPACE_DIR
        WORKSPACE_DIR = new_workspace

        logger.info(f"New Workspace: {WORKSPACE_DIR}")

        result = {
            "status": "success",
            "message": f"Workspace setup successful: {workspace_path}",
            "new_workspace": str(WORKSPACE_DIR),
        }

        log_operation(
            "set_workspace",
            {
                "old_workspace": str(old_workspace) if old_workspace else None,
                "new_workspace": str(WORKSPACE_DIR),
                "workspace_alignment": "plan_file_parent/generate_code",
            },
        )

        return json.dumps(result, ensure_ascii=False, indent=2)

    except Exception as e:
        result = {
            "status": "error",
            "message": f"è®¾ç½®å·¥ä½œç©ºé—´å¤±è´¥: {str(e)}",
            "workspace_path": workspace_path,
        }
        log_operation(
            "set_workspace_error", {"workspace_path": workspace_path, "error": str(e)}
        )
        return json.dumps(result, ensure_ascii=False, indent=2)


@mcp.tool()
async def get_operation_history(last_n: int = 10) -> str:
    """
    è·å–æ“ä½œå†å²

    Args:
        last_n: è¿”å›æœ€è¿‘çš„Nä¸ªæ“ä½œ

    Returns:
        æ“ä½œå†å²çš„JSONå­—ç¬¦ä¸²
    """
    try:
        recent_history = (
            OPERATION_HISTORY[-last_n:] if last_n > 0 else OPERATION_HISTORY
        )

        result = {
            "status": "success",
            "total_operations": len(OPERATION_HISTORY),
            "returned_operations": len(recent_history),
            "workspace": str(WORKSPACE_DIR) if WORKSPACE_DIR else None,
            "history": recent_history,
        }

        return json.dumps(result, ensure_ascii=False, indent=2)

    except Exception as e:
        result = {"status": "error", "message": f"è·å–æ“ä½œå†å²å¤±è´¥: {str(e)}"}
        return json.dumps(result, ensure_ascii=False, indent=2)


# ==================== æœåŠ¡å™¨åˆå§‹åŒ– ====================


def main():
    """å¯åŠ¨MCPæœåŠ¡å™¨"""
    print("ğŸš€ Code Implementation MCP Server")
    print("ğŸ“ è®ºæ–‡ä»£ç å¤ç°å·¥å…·æœåŠ¡å™¨ / Paper Code Implementation Tool Server")
    print("")
    print("Available tools / å¯ç”¨å·¥å…·:")
    # print("  â€¢ read_file           - è¯»å–æ–‡ä»¶å†…å®¹ / Read file contents")
    print("  â€¢ read_code_mem       - è¯»å–ä»£ç æ‘˜è¦ / Read code summary from implement_code_summary.md")
    print("  â€¢ write_file          - å†™å…¥æ–‡ä»¶å†…å®¹ / Write file contents")
    print("  â€¢ execute_python      - æ‰§è¡ŒPythonä»£ç  / Execute Python code")
    print("  â€¢ execute_bash        - æ‰§è¡Œbashå‘½ä»¤ / Execute bash commands")
    print("  â€¢ search_code         - æœç´¢ä»£ç æ¨¡å¼ / Search code patterns")
    print("  â€¢ get_file_structure  - è·å–æ–‡ä»¶ç»“æ„ / Get file structure")
    print("  â€¢ set_workspace       - è®¾ç½®å·¥ä½œç©ºé—´ / Set workspace")
    print("  â€¢ get_operation_history - è·å–æ“ä½œå†å² / Get operation history")
    print("")
    print("ğŸ”§ Server starting...")

    # åˆå§‹åŒ–é»˜è®¤å·¥ä½œç©ºé—´
    initialize_workspace()
    
    # å¯åŠ¨æœåŠ¡å™¨
    mcp.run()


if __name__ == "__main__":
    main()
