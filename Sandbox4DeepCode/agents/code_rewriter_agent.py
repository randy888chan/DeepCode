#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä»£ç é‡å†™ä»£ç†
åŠŸèƒ½ï¼šæŒ‰ç…§åŸrepoç»“æ„é‡å†™æ¯ä¸ªä»£ç æ–‡ä»¶ï¼Œæ·»åŠ è¯¦ç»†logä¿¡æ¯å’Œå¿…è¦çš„æµ‹è¯•è¾“å…¥ï¼Œç”Ÿæˆä¾èµ–åˆ—è¡¨

"""

import os
import shutil
import logging
import re
import time
import json
from typing import Dict, Any, List, Set
from concurrent.futures import ThreadPoolExecutor, as_completed
from agents.base_agent import BaseAgent
from utils.file_utils import read_file_content
from utils.colored_logging import log_checkpoint, log_detailed


class CodeRewriterAgent(BaseAgent):
    """ä»£ç é‡å†™ä»£ç† - é‡å†™ä»£ç æ–‡ä»¶æ·»åŠ logå’Œæµ‹è¯•è¾“å…¥"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.test_repo_suffix = "_test_sandbox"
        self.dependencies = set()
        
    def process(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        å¤„ç†ä»£ç é‡å†™è¯·æ±‚
        
        Args:
            input_data: åŒ…å«repo_path, structure_analysisçš„å­—å…¸
            
        Returns:
            ä»£ç é‡å†™ç»“æœ
        """
        try:
            if not self.validate_input(input_data):
                return self.handle_error(ValueError("è¾“å…¥æ•°æ®æ— æ•ˆ"))
            
            repo_path = input_data['repo_path']
            structure_analysis = input_data['structure_analysis']
            
            # åˆ›å»ºæµ‹è¯•repoç›®å½•
            test_repo_path = self._create_test_repo_directory(repo_path)
            
            # å¤åˆ¶å’Œé‡å†™ä»£ç æ–‡ä»¶
            rewritten_files = self._rewrite_code_files(repo_path, test_repo_path, structure_analysis)
            
            # ç”Ÿæˆrequirements.txt
            requirements_path = self._generate_requirements_file(test_repo_path)
            
            # ç”Ÿæˆé¡¹ç›®æ‘˜è¦
            project_summary = self._generate_project_summary(rewritten_files, structure_analysis)
            
            result = {
                'test_repo_path': test_repo_path,
                'rewritten_files': rewritten_files,
                'requirements_path': requirements_path,
                'dependencies': list(self.dependencies),
                'total_files': len(rewritten_files),
                'log_points_added': sum(f.get('log_points', 0) for f in rewritten_files),
                'project_summary': project_summary
            }
            
            return self.format_output(result)
            
        except Exception as e:
            return self.handle_error(e)
    
    def _create_test_repo_directory(self, repo_path: str) -> str:
        """åˆ›å»ºæµ‹è¯•repoç›®å½•"""
        try:
            repo_name = os.path.basename(repo_path.rstrip('/\\'))
            # å°†æµ‹è¯•repoä¿å­˜åˆ°outputç›®å½•ä¸‹
            output_dir = os.path.join(os.getcwd(), 'output')
            os.makedirs(output_dir, exist_ok=True)
            test_repo_path = os.path.join(output_dir, f"{repo_name}{self.test_repo_suffix}")
            
            # å¦‚æœç›®å½•å·²å­˜åœ¨ï¼Œå¼ºåˆ¶åˆ é™¤ï¼ˆåŒ…æ‹¬è™šæ‹Ÿç¯å¢ƒç­‰é¡½å›ºç›®å½•ï¼‰
            if os.path.exists(test_repo_path):
                self._force_remove_directory(test_repo_path)
            
            # åˆ›å»ºæ–°ç›®å½•
            os.makedirs(test_repo_path)
            logging.info(f"åˆ›å»ºæµ‹è¯•repoç›®å½•: {test_repo_path}")
            
            return test_repo_path
            
        except Exception as e:
            logging.error(f"åˆ›å»ºæµ‹è¯•repoç›®å½•å¤±è´¥: {str(e)}")
            raise
    
    def _force_remove_directory(self, dir_path: str):
        """å¼ºåˆ¶åˆ é™¤ç›®å½•ï¼Œå¤„ç†è™šæ‹Ÿç¯å¢ƒç­‰é¡½å›ºç›®å½•"""
        import stat
        import time
        
        def handle_remove_readonly(func, path, exc):
            """å¤„ç†åªè¯»æ–‡ä»¶åˆ é™¤"""
            if os.path.exists(path):
                os.chmod(path, stat.S_IWRITE)
                func(path)
        
        try:
            # é¦–å…ˆå°è¯•æ­£å¸¸åˆ é™¤
            shutil.rmtree(dir_path)
            logging.info(f"æˆåŠŸåˆ é™¤ç›®å½•: {dir_path}")
        except Exception as e1:
            logging.warning(f"æ­£å¸¸åˆ é™¤å¤±è´¥: {e1}ï¼Œå°è¯•å¼ºåˆ¶åˆ é™¤...")
            try:
                # å¤„ç†åªè¯»æ–‡ä»¶ï¼Œç„¶åé‡è¯•
                shutil.rmtree(dir_path, onerror=handle_remove_readonly)
                logging.info(f"å¼ºåˆ¶åˆ é™¤æˆåŠŸ: {dir_path}")
            except Exception as e2:
                logging.warning(f"å¼ºåˆ¶åˆ é™¤å¤±è´¥: {e2}ï¼Œå°è¯•ç³»ç»Ÿå‘½ä»¤...")
                try:
                    # æœ€åå°è¯•ä½¿ç”¨ç³»ç»Ÿå‘½ä»¤
                    import subprocess
                    if os.name == 'nt':  # Windows
                        subprocess.run(['rmdir', '/s', '/q', dir_path], check=True, shell=True)
                    else:  # Unix/Linux/Mac
                        subprocess.run(['rm', '-rf', dir_path], check=True)
                    logging.info(f"ç³»ç»Ÿå‘½ä»¤åˆ é™¤æˆåŠŸ: {dir_path}")
                except Exception as e3:
                    logging.error(f"æ‰€æœ‰åˆ é™¤æ–¹æ³•éƒ½å¤±è´¥: {e3}")
                    # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œæ·»åŠ æ—¶é—´æˆ³é¿å…å†²çª
                    import random
                    timestamp = int(time.time())
                    new_name = f"{dir_path}_backup_{timestamp}_{random.randint(1000,9999)}"
                    os.rename(dir_path, new_name)
                    logging.warning(f"æ— æ³•åˆ é™¤ï¼Œå·²é‡å‘½åä¸º: {new_name}")
    
    def _rewrite_code_files(self, repo_path: str, test_repo_path: str, structure_analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """é‡å†™ä»£ç æ–‡ä»¶ï¼ˆå¹¶å‘æ‰§è¡ŒPythonæ–‡ä»¶é‡å†™ï¼‰"""
        try:
            rewritten_files = []
            python_files = []
            other_files = []
            
            log_checkpoint("æ‰«æé¡¹ç›®æ–‡ä»¶")
            
            # éå†åŸrepoçš„æ‰€æœ‰æ–‡ä»¶ï¼Œåˆ†ç±»æ”¶é›†
            for root, dirs, files in os.walk(repo_path):
                # è·³è¿‡å¸¸è§çš„éä»£ç ç›®å½•
                dirs[:] = [d for d in dirs if not d.startswith('.') and d not in ['__pycache__', 'node_modules', 'venv', 'env', '.git']]
                
                for file in files:
                    source_path = os.path.join(root, file)
                    rel_path = os.path.relpath(source_path, repo_path)
                    target_path = os.path.join(test_repo_path, rel_path)
                    
                    # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
                    os.makedirs(os.path.dirname(target_path), exist_ok=True)
                    
                    if file.endswith('.py'):
                        # Pythonæ–‡ä»¶æ·»åŠ åˆ°å¹¶å‘å¤„ç†åˆ—è¡¨
                        python_files.append((source_path, target_path, rel_path))
                    else:
                        # å…¶ä»–æ–‡ä»¶å…ˆæ”¶é›†èµ·æ¥
                        other_files.append((source_path, target_path, rel_path, file))
            
            log_detailed("INFO", f"å‘ç° {len(python_files)} ä¸ªPythonæ–‡ä»¶ï¼Œ{len(other_files)} ä¸ªå…¶ä»–æ–‡ä»¶")
            
            # 1. å¹¶å‘å¤„ç†Pythonæ–‡ä»¶
            if python_files:
                log_checkpoint("å¼€å§‹å¹¶å‘é‡å†™Pythonæ–‡ä»¶")
                python_results = self._concurrent_rewrite_python_files(python_files, structure_analysis)
                rewritten_files.extend(python_results)
                log_checkpoint("Pythonæ–‡ä»¶å¹¶å‘é‡å†™å®Œæˆ", f"å¤„ç†äº† {len(python_results)} ä¸ªæ–‡ä»¶")
            
            # 2. ä¸²è¡Œå¤„ç†å…¶ä»–æ–‡ä»¶ï¼ˆç®€å•å¤åˆ¶æ“ä½œæ— éœ€å¹¶å‘ï¼‰
            if other_files:
                log_checkpoint("å¤„ç†å…¶ä»–æ–‡ä»¶")
                for source_path, target_path, rel_path, file in other_files:
                    if file in ['requirements.txt', 'setup.py', 'pyproject.toml', 'setup.cfg']:
                        # é…ç½®æ–‡ä»¶ç›´æ¥å¤åˆ¶å¹¶åˆ†æä¾èµ–
                        shutil.copy2(source_path, target_path)
                        self._extract_dependencies_from_config(source_path, file)
                        rewritten_files.append({
                            'original_path': rel_path,
                            'target_path': target_path,
                            'type': 'config',
                            'copied': True
                        })
                    else:
                        # å…¶ä»–æ–‡ä»¶ç›´æ¥å¤åˆ¶
                        shutil.copy2(source_path, target_path)
                        rewritten_files.append({
                            'original_path': rel_path,
                            'target_path': target_path,
                            'type': 'other',
                            'copied': True
                        })
                log_checkpoint("å…¶ä»–æ–‡ä»¶å¤„ç†å®Œæˆ", f"å¤„ç†äº† {len(other_files)} ä¸ªæ–‡ä»¶")
            
            return rewritten_files
            
        except Exception as e:
            logging.error(f"é‡å†™ä»£ç æ–‡ä»¶å¤±è´¥: {str(e)}")
            raise
    
    def _concurrent_rewrite_python_files(self, python_files: List[tuple], structure_analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """å¹¶å‘é‡å†™Pythonæ–‡ä»¶"""
        try:
            results = []
            total_files = len(python_files)
            
            # æ§åˆ¶å¹¶å‘æ•°é‡ï¼Œé¿å…APIé™åˆ¶
            # ä»é…ç½®ä¸­è·å–å¹¶å‘æ•°é‡ï¼Œé»˜è®¤ä¸º15
            execution_config = self.config.get('execution', {})
            config_workers = execution_config.get('concurrent_workers', 15)
            max_workers = min(config_workers, total_files)
            log_detailed("INFO", f"ä½¿ç”¨ {max_workers} ä¸ªå¹¶å‘çº¿ç¨‹å¤„ç† {total_files} ä¸ªPythonæ–‡ä»¶")
            
            start_time = time.time()
            
            def process_single_file(file_info):
                """å¤„ç†å•ä¸ªPythonæ–‡ä»¶çš„åŒ…è£…å‡½æ•°"""
                source_path, target_path, rel_path = file_info
                try:
                    log_detailed("DEBUG", f"å¼€å§‹å¤„ç†: {rel_path}")
                    result = self._rewrite_python_file(source_path, target_path, structure_analysis)
                    log_detailed("DEBUG", f"å®Œæˆå¤„ç†: {rel_path}")
                    return result
                except Exception as e:
                    log_detailed("ERROR", f"å¤„ç†æ–‡ä»¶å¤±è´¥ {rel_path}: {str(e)}")
                    # è¿”å›é”™è¯¯ç»“æœï¼Œè€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
                    return {
                        'original_path': rel_path,
                        'target_path': target_path,
                        'type': 'python',
                        'error': str(e)
                    }
            
            # ä½¿ç”¨ThreadPoolExecutorè¿›è¡Œå¹¶å‘å¤„ç†
            with ThreadPoolExecutor(max_workers=max_workers, thread_name_prefix="PyRewriter") as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                future_to_file = {
                    executor.submit(process_single_file, file_info): file_info[2]  # file_info[2] æ˜¯ rel_path
                    for file_info in python_files
                }
                
                completed_count = 0
                
                # æ”¶é›†ç»“æœ
                for future in as_completed(future_to_file):
                    rel_path = future_to_file[future]
                    completed_count += 1
                    
                    try:
                        result = future.result()
                        results.append(result)
                        
                        # æ˜¾ç¤ºè¿›åº¦
                        if completed_count % 5 == 0 or completed_count == total_files:
                            progress = (completed_count / total_files) * 100
                            elapsed = time.time() - start_time
                            log_detailed("INFO", f"è¿›åº¦: {completed_count}/{total_files} ({progress:.1f}%) - ç”¨æ—¶: {elapsed:.1f}s")
                        
                    except Exception as e:
                        log_detailed("ERROR", f"è·å–ç»“æœå¤±è´¥ {rel_path}: {str(e)}")
                        results.append({
                            'original_path': rel_path,
                            'target_path': '',
                            'type': 'python',
                            'error': str(e)
                        })
            
            total_time = time.time() - start_time
            successful_count = len([r for r in results if 'error' not in r])
            failed_count = total_files - successful_count
            
            log_detailed("INFO", f"å¹¶å‘å¤„ç†å®Œæˆ: æˆåŠŸ {successful_count}ï¼Œå¤±è´¥ {failed_count}ï¼Œæ€»ç”¨æ—¶ {total_time:.2f}s")
            
            # ğŸ†• èšåˆæ‰€æœ‰Pythonæ–‡ä»¶çš„ä¾èµ–
            self._aggregate_dependencies_from_results(results)
            
            return results
            
        except Exception as e:
            logging.error(f"å¹¶å‘é‡å†™Pythonæ–‡ä»¶å¤±è´¥: {str(e)}")
            # å¦‚æœå¹¶å‘å¤„ç†å¤±è´¥ï¼Œå›é€€åˆ°ä¸²è¡Œå¤„ç†
            log_detailed("WARNING", "å¹¶å‘å¤„ç†å¤±è´¥ï¼Œå›é€€åˆ°ä¸²è¡Œå¤„ç†")
            return self._fallback_serial_rewrite(python_files, structure_analysis)
    
    def _fallback_serial_rewrite(self, python_files: List[tuple], structure_analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """å¹¶å‘å¤„ç†å¤±è´¥æ—¶çš„ä¸²è¡Œå¤„ç†å›é€€æ–¹æ¡ˆ"""
        try:
            results = []
            for source_path, target_path, rel_path in python_files:
                try:
                    result = self._rewrite_python_file(source_path, target_path, structure_analysis)
                    results.append(result)
                except Exception as e:
                    logging.error(f"ä¸²è¡Œå¤„ç†æ–‡ä»¶å¤±è´¥ {rel_path}: {str(e)}")
                    results.append({
                        'original_path': rel_path,
                        'target_path': target_path,
                        'type': 'python',
                        'error': str(e),
                        'dependencies': []  # ğŸ†• é”™è¯¯æƒ…å†µä¸‹çš„ç©ºä¾èµ–åˆ—è¡¨
                    })
            
            # ğŸ†• èšåˆä¸²è¡Œå¤„ç†çš„ä¾èµ–
            self._aggregate_dependencies_from_results(results)
            
            return results
        except Exception as e:
            logging.error(f"ä¸²è¡Œå›é€€å¤„ç†å¤±è´¥: {str(e)}")
            return []
    
    def _rewrite_python_file(self, source_path: str, target_path: str, structure_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """é‡å†™Pythonæ–‡ä»¶ï¼Œæ·»åŠ logå’Œæµ‹è¯•è¾“å…¥"""
        try:
            original_content = read_file_content(source_path)
            if not original_content:
                # ç©ºæ–‡ä»¶ç›´æ¥å¤åˆ¶
                shutil.copy2(source_path, target_path)
                return {
                    'original_path': os.path.relpath(source_path, os.path.dirname(source_path)),
                    'target_path': target_path,
                    'type': 'python',
                    'empty': True
                }
            
            # ğŸ†• ä½¿ç”¨æ–°çš„LLMé‡å†™æ–¹æ³•ï¼ˆJSONæ ¼å¼ï¼‰
            llm_result = self._llm_rewrite_code_with_deps(original_content, source_path, structure_analysis)
            
            if llm_result.get('success'):
                # LLMæˆåŠŸè¿”å›JSONæ ¼å¼ç»“æœ
                rewritten_content = llm_result['code']
                
                # åå¤„ç†ï¼šç¡®ä¿å¯¼å…¥æ­£ç¡®
                rewritten_content = self._post_process_imports(rewritten_content, source_path)
                
                # å†™å…¥é‡å†™åçš„å†…å®¹
                with open(target_path, 'w', encoding='utf-8') as f:
                    f.write(rewritten_content)
                
                # åˆ†æé‡å†™ç»“æœ
                log_points = self._count_log_points(rewritten_content)
                
                return {
                    'original_path': os.path.relpath(source_path, os.path.dirname(source_path)),
                    'target_path': target_path,
                    'type': 'python',
                    'rewritten': True,
                    'log_points': log_points,
                    'dependencies': llm_result.get('packages', []),  # ğŸ¯ LLMç›´æ¥æä¾›çš„å‡†ç¡®åŒ…å
                    'explanation': llm_result.get('explanation', ''),
                    'llm_method': True,  # æ ‡è®°ä½¿ç”¨äº†LLMæ–¹æ³•
                    'json_attempt': llm_result.get('attempt', 1),
                    'original_lines': len(original_content.split('\n')),
                    'rewritten_lines': len(rewritten_content.split('\n'))
                }
            else:
                # LLM JSONæ–¹æ³•å¤±è´¥ï¼Œå›é€€åˆ°fallbackå¢å¼º
                logging.warning(f"LLM JSONæ–¹æ³•å¤±è´¥ï¼Œå›é€€åˆ°åŸºç¡€å¢å¼º: {source_path}")
                enhanced_content = self._add_basic_logging(original_content)
                with open(target_path, 'w', encoding='utf-8') as f:
                    f.write(enhanced_content)
                
                # ä½¿ç”¨ç®€åŒ–çš„é™æ€åˆ†æä½œä¸ºfallbackï¼ˆä¿ç•™ä½œä¸ºå…œåº•ï¼‰
                fallback_dependencies = self._analyze_file_dependencies_fallback(original_content, enhanced_content)
                
                return {
                    'original_path': os.path.relpath(source_path, os.path.dirname(source_path)),
                    'target_path': target_path,
                    'type': 'python',
                    'fallback_enhanced': True,
                    'log_points': self._count_log_points(enhanced_content),
                    'dependencies': list(fallback_dependencies),  # fallbackä¾èµ–åˆ†æ
                    'llm_error': llm_result.get('error', 'Unknown'),
                    'llm_method': False
                }
                
        except Exception as e:
            logging.error(f"é‡å†™Pythonæ–‡ä»¶å¤±è´¥ {source_path}: {str(e)}")
            # å¤±è´¥æ—¶ç›´æ¥å¤åˆ¶åŸæ–‡ä»¶
            shutil.copy2(source_path, target_path)
            
            # ğŸ†• å³ä½¿å¤±è´¥ä¹Ÿå°è¯•åˆ†æåŸæ–‡ä»¶çš„ä¾èµ–ï¼ˆfallbackï¼‰
            try:
                original_content = read_file_content(source_path) or ""
                error_dependencies = self._analyze_file_dependencies_fallback(original_content, original_content)
            except:
                error_dependencies = set()
            
            return {
                'original_path': os.path.relpath(source_path, os.path.dirname(source_path)),
                'target_path': target_path,
                'type': 'python',
                'error': str(e),
                'dependencies': list(error_dependencies)  # ğŸ†• é”™è¯¯æƒ…å†µä¸‹çš„ä¾èµ–åˆ—è¡¨
            }
    
    def _llm_rewrite_code_with_deps(self, original_content: str, file_path: str, structure_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """ä½¿ç”¨LLMé‡å†™ä»£ç å¹¶åŒæ—¶æä¾›ä¾èµ–åŒ…ä¿¡æ¯"""
        try:
            # è·å–æ–‡ä»¶åœ¨é¡¹ç›®ä¸­çš„è§’è‰²
            file_role = self._determine_file_role(file_path, structure_analysis)
            
            # é¢„å¤„ç†ï¼šä¿®å¤ç›¸å¯¹å¯¼å…¥
            preprocessed_content = self._fix_relative_imports(original_content, file_path)
            
            messages = [
                self.create_system_message(
                    "Pythonä»£ç é‡å†™ä¸“å®¶",
                    "é‡å†™Pythonä»£ç å¹¶ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼ŒåŒ…å«é‡å†™åçš„ä»£ç å’Œæ‰€éœ€çš„ä¾èµ–åŒ…ä¿¡æ¯ã€‚"
                ),
                self.create_user_message(
                    f"""è¯·é‡å†™ä»¥ä¸‹Pythonä»£ç ï¼Œå¹¶ä»¥JSONæ ¼å¼è¿”å›ç»“æœã€‚

é‡å†™è¦æ±‚ï¼š
1. **ä¿æŒåŸæœ‰åŠŸèƒ½å®Œå…¨ä¸å˜**
2. **ä¿®å¤å¯¼å…¥é—®é¢˜**ï¼šå°†ç›¸å¯¹å¯¼å…¥è½¬æ¢ä¸ºç»å¯¹å¯¼å…¥
3. **æ·»åŠ è¯¦ç»†çš„æ—¥å¿—è®°å½•**ï¼š
   - åœ¨æ¯ä¸ªå‡½æ•°å…¥å£æ·»åŠ  logging.info(f"[LOG] ENTER function_name: {{å‚æ•°ä¿¡æ¯}}")
   - åœ¨å…³é”®å˜é‡èµ‹å€¼å¤„æ·»åŠ  logging.info(f"[LOG] VARIABLE variable_name = {{value}}")
   - åœ¨å‡½æ•°é€€å‡ºå‰æ·»åŠ  logging.info(f"[LOG] EXIT function_name: {{è¿”å›å€¼ä¿¡æ¯}}")
   - åœ¨å¼‚å¸¸å¤„ç†å¤„æ·»åŠ  logging.error(f"[LOG] ERROR in function_name: {{é”™è¯¯ä¿¡æ¯}}")
4. **æ·»åŠ å¿…è¦çš„æµ‹è¯•è¾“å…¥**ï¼šä¸ºä¸»å…¥å£æ–‡ä»¶æ·»åŠ åˆç†çš„æµ‹è¯•æ•°æ®
5. **å¦‚æœé€‚åˆï¼Œå¯ä»¥æ·»åŠ æ€§èƒ½ç›‘æ§**ï¼ˆå¦‚psutilç›‘æ§å†…å­˜/CPUï¼‰

æ–‡ä»¶è·¯å¾„ï¼š{file_path}
æ–‡ä»¶è§’è‰²ï¼š{file_role}

åŸå§‹ä»£ç ï¼š
```python
{preprocessed_content}
```

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼š

```json
{{
    "rewritten_code": "é‡å†™åçš„å®Œæ•´Pythonä»£ç ï¼ˆåŒ…å«æ‰€æœ‰importè¯­å¥ï¼‰",
    "required_packages": ["package1", "package2"],
    "explanation": "é‡å†™è¯´æ˜å’Œæ–°å¢åŠŸèƒ½æè¿°"
}}
```

é‡è¦è¯´æ˜ï¼š
- required_packageså¿…é¡»æ˜¯å‡†ç¡®çš„pipå®‰è£…åŒ…åï¼ˆä¸æ˜¯importåï¼‰
- ä¾‹å¦‚ï¼šcv2å¯¹åº”opencv-pythonï¼ŒPILå¯¹åº”Pillowï¼Œsklearnå¯¹åº”scikit-learn
- åªåŒ…å«ç¬¬ä¸‰æ–¹åŒ…ï¼Œä¸è¦åŒ…å«æ ‡å‡†åº“ï¼ˆå¦‚os, sys, loggingç­‰ï¼‰
- rewritten_codeå¿…é¡»æ˜¯å®Œæ•´å¯è¿è¡Œçš„ä»£ç 
""")
            ]
            
            response = self.call_llm(messages, max_tokens=8000)
            
            # ä½¿ç”¨JSONæå–å’Œé‡è¯•æœºåˆ¶
            return self._extract_json_with_retry(response, file_path)
                
        except Exception as e:
            logging.error(f"LLMé‡å†™ä»£ç å¤±è´¥ {file_path}: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'fallback_response': None
            }
    
    def _fix_relative_imports(self, content: str, file_path: str) -> str:
        """ä¿®å¤ç›¸å¯¹å¯¼å…¥è·¯å¾„"""
        try:
            lines = content.split('\n')
            fixed_lines = []
            
            for line in lines:
                # å¤„ç†ç›¸å¯¹å¯¼å…¥
                if re.match(r'^\s*from\s+\.+', line):
                    # åŒ¹é…ç±»ä¼¼ "from ..models import xxx" æˆ– "from .utils import xxx" çš„æ¨¡å¼
                    match = re.match(r'(\s*)from\s+(\.+)(\w+(?:\.\w+)*)\s+import\s+(.+)', line)
                    if match:
                        indent, dots, module_path, imports = match.groups()
                        
                        # æ ¹æ®ç‚¹çš„æ•°é‡å’Œæ–‡ä»¶è·¯å¾„è®¡ç®—ç»å¯¹å¯¼å…¥è·¯å¾„
                        abs_import = self._convert_to_absolute_import(file_path, dots, module_path)
                        if abs_import:
                            fixed_line = f"{indent}from {abs_import} import {imports}"
                            fixed_lines.append(fixed_line)
                            logging.info(f"ä¿®å¤ç›¸å¯¹å¯¼å…¥: {line.strip()} -> {fixed_line.strip()}")
                            continue
                
                # ä¿æŒå…¶ä»–è¡Œä¸å˜
                fixed_lines.append(line)
            
            return '\n'.join(fixed_lines)
            
        except Exception as e:
            logging.error(f"ä¿®å¤ç›¸å¯¹å¯¼å…¥å¤±è´¥: {str(e)}")
            return content
    
    def _convert_to_absolute_import(self, file_path: str, dots: str, module_path: str) -> str:
        """å°†ç›¸å¯¹å¯¼å…¥è½¬æ¢ä¸ºç»å¯¹å¯¼å…¥"""
        try:
            # å¯¹äºç›¸å¯¹å¯¼å…¥ï¼Œä¼˜å…ˆä½¿ç”¨åŸºäºsrcç›®å½•çš„ç®€å•æ˜ å°„
            if dots == '.':
                # åŒçº§å¯¼å…¥ï¼Œå¦‚ from .models import xxx
                if 'src' in file_path:
                    return f"src.{module_path}" if module_path else "src"
                else:
                    return module_path if module_path else ""
            elif dots == '..':
                # ä¸Šçº§å¯¼å…¥ï¼Œå¦‚ from ..models import xxx
                if 'src' in file_path:
                    # ä»srcç›®å½•å¼€å§‹çš„ç»å¯¹å¯¼å…¥
                    return f"src.{module_path}" if module_path else "src"
                else:
                    # å¯¹äºésrcç›®å½•çš„æ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨æ¨¡å—è·¯å¾„
                    return module_path if module_path else ""
            else:
                # å¤šçº§ä¸Šå‡ï¼Œç»Ÿä¸€ä½¿ç”¨srcå‰ç¼€
                if module_path:
                    return f"src.{module_path}"
                else:
                    return "src"
            
        except Exception as e:
            logging.error(f"è½¬æ¢ç»å¯¹å¯¼å…¥å¤±è´¥: {str(e)}")
            return None
    
    def _post_process_imports(self, content: str, file_path: str) -> str:
        """åå¤„ç†å¯¼å…¥è¯­å¥ï¼Œç¡®ä¿æ­£ç¡®æ€§"""
        try:
            lines = content.split('\n')
            processed_lines = []
            has_sys_path = False
            import_section_ended = False
            in_multiline_import = False
            multiline_import_buffer = []
            
            i = 0
            while i < len(lines):
                line = lines[i]
                
                # æ£€æŸ¥æ˜¯å¦å·²æ·»åŠ sys.pathè®¾ç½®
                if 'sys.path.insert' in line:
                    has_sys_path = True
                
                # æ£€æŸ¥æ˜¯å¦å¼€å§‹å¤šè¡Œå¯¼å…¥
                if (line.strip().startswith('from ') or line.strip().startswith('import ')) and '(' in line and ')' not in line:
                    # å¼€å§‹å¤šè¡Œå¯¼å…¥ï¼Œæ”¶é›†æ‰€æœ‰ç›¸å…³è¡Œ
                    in_multiline_import = True
                    multiline_import_buffer = [line]
                    i += 1
                    
                    # ç»§ç»­æ”¶é›†å¤šè¡Œimportçš„æ‰€æœ‰è¡Œ
                    while i < len(lines) and in_multiline_import:
                        current_line = lines[i]
                        multiline_import_buffer.append(current_line)
                        
                        if ')' in current_line:
                            in_multiline_import = False
                        i += 1
                    
                    # å°†å®Œæ•´çš„å¤šè¡Œimportæ·»åŠ åˆ°å¤„ç†åçš„è¡Œä¸­
                    processed_lines.extend(multiline_import_buffer)
                    multiline_import_buffer = []
                    continue
                    
                processed_lines.append(line)
                
                # æ£€æŸ¥å¯¼å…¥åŒºåŸŸæ˜¯å¦ç»“æŸï¼ˆåªæœ‰å½“ä¸åœ¨å¤šè¡Œå¯¼å…¥ä¸­æ—¶æ‰æ£€æŸ¥ï¼‰
                if not in_multiline_import and not (line.startswith('import ') or line.startswith('from ') or line.strip() == '' or line.strip().startswith('#')):
                    import_section_ended = True
                
                # åœ¨å¯¼å…¥åŒºåŸŸç»“æŸåæ·»åŠ sys.pathè®¾ç½®ï¼ˆå¦‚æœéœ€è¦ä¸”è¿˜æ²¡æœ‰ï¼‰
                if not has_sys_path and import_section_ended and 'src' in content:
                    # åœ¨å½“å‰ä½ç½®æ’å…¥sys.pathè®¾ç½®
                    processed_lines.insert(-1, '')  # ç©ºè¡Œåˆ†éš”
                    processed_lines.insert(-1, 'import os')
                    processed_lines.insert(-1, 'import sys')
                    processed_lines.insert(-1, "sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))")
                    processed_lines.insert(-1, '')  # ç©ºè¡Œåˆ†éš”
                    has_sys_path = True
                
                i += 1
            
            return '\n'.join(processed_lines)
            
        except Exception as e:
            logging.error(f"åå¤„ç†å¯¼å…¥å¤±è´¥: {str(e)}")
            return content

    def _determine_file_role(self, file_path: str, structure_analysis: Dict[str, Any]) -> str:
        """ç¡®å®šæ–‡ä»¶åœ¨é¡¹ç›®ä¸­çš„è§’è‰²"""
        try:
            filename = os.path.basename(file_path)
            rel_path = file_path
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯å…¥å£æ–‡ä»¶
            recommended_entry = structure_analysis.get('recommended_entry', '')
            if recommended_entry and recommended_entry in rel_path:
                return "ä¸»å…¥å£æ–‡ä»¶"
            
            # æ£€æŸ¥å¸¸è§è§’è‰²
            if filename in ['main.py', 'run.py', 'app.py']:
                return "ä¸»å…¥å£æ–‡ä»¶"
            elif filename.startswith('test_'):
                return "æµ‹è¯•æ–‡ä»¶"
            elif filename == '__init__.py':
                return "åŒ…åˆå§‹åŒ–æ–‡ä»¶"
            elif 'config' in filename.lower():
                return "é…ç½®æ–‡ä»¶"
            elif 'util' in filename.lower() or 'helper' in filename.lower():
                return "å·¥å…·æ–‡ä»¶"
            else:
                return "æ™®é€šæ¨¡å—æ–‡ä»¶"
                
        except Exception as e:
            logging.debug(f"ç¡®å®šæ–‡ä»¶è§’è‰²å¤±è´¥: {str(e)}")
            return "æ™®é€šæ–‡ä»¶"
    
    def _add_basic_logging(self, content: str) -> str:
        """æ·»åŠ åŸºç¡€æ—¥å¿—åŠŸèƒ½ï¼ˆä½œä¸ºLLMé‡å†™çš„åå¤‡æ–¹æ¡ˆï¼‰"""
        try:
            lines = content.split('\n')
            enhanced_lines = []
            
            # æ·»åŠ loggingå¯¼å…¥
            import_added = False
            for i, line in enumerate(lines):
                enhanced_lines.append(line)
                
                # åœ¨ç¬¬ä¸€ä¸ªimportåæ·»åŠ logging
                if not import_added and (line.startswith('import ') or line.startswith('from ')):
                    if i + 1 >= len(lines) or not (lines[i + 1].startswith('import ') or lines[i + 1].startswith('from ')):
                        enhanced_lines.append('import logging')
                        enhanced_lines.append('logging.basicConfig(level=logging.INFO)')
                        import_added = True
                
                # åœ¨å‡½æ•°å®šä¹‰åæ·»åŠ è¿›å…¥æ—¥å¿—
                if line.strip().startswith('def ') and ':' in line:
                    func_name = line.split('def ')[1].split('(')[0]
                    indent = len(line) - len(line.lstrip())
                    enhanced_lines.append(' ' * (indent + 4) + f'logging.info(f"[LOG] ENTER {func_name}")')
            
            # å¦‚æœæ²¡æœ‰æ·»åŠ è¿‡loggingå¯¼å…¥ï¼Œåœ¨å¼€å¤´æ·»åŠ 
            if not import_added:
                enhanced_lines.insert(0, 'import logging')
                enhanced_lines.insert(1, 'logging.basicConfig(level=logging.INFO)')
            
            return '\n'.join(enhanced_lines)
            
        except Exception as e:
            logging.error(f"æ·»åŠ åŸºç¡€æ—¥å¿—å¤±è´¥: {str(e)}")
            return content
    
    def _count_log_points(self, content: str) -> int:
        """ç»Ÿè®¡æ—¥å¿—ç‚¹æ•°é‡"""
        try:
            return len(re.findall(r'\[LOG\]', content))
        except:
            return 0
    
    def _extract_json_with_retry(self, response: str, file_path: str, max_retries: int = 3) -> Dict[str, Any]:
        """æå–JSONå†…å®¹ï¼ŒåŒ…å«é‡è¯•ç­–ç•¥"""
        
        for attempt in range(max_retries):
            try:
                # å°è¯•å¤šç§JSONæå–æ¨¡å¼
                json_patterns = [
                    r'```json\s*(\{.*?\})\s*```',  # æ ‡å‡†ä»£ç å—
                    r'```\s*(\{.*?\})\s*```',      # ç®€å•ä»£ç å—
                    r'(\{[^{}]*"rewritten_code"[^{}]*"required_packages"[^{}]*\})',  # åŒ…å«å…³é”®å­—æ®µ
                    r'(\{.*?"rewritten_code".*?"required_packages".*?\})',  # æ›´å®½æ¾åŒ¹é…
                ]
                
                json_str = None
                for pattern in json_patterns:
                    json_match = re.search(pattern, response, re.DOTALL)
                    if json_match:
                        json_str = json_match.group(1)
                        break
                
                if json_str:
                    # æ¸…ç†JSONå­—ç¬¦ä¸²
                    json_str = json_str.strip()
                    result = json.loads(json_str)
                    
                    # éªŒè¯å¿…è¦å­—æ®µ
                    if 'rewritten_code' in result and 'required_packages' in result:
                        return {
                            'success': True,
                            'code': result['rewritten_code'],
                            'packages': result.get('required_packages', []),
                            'explanation': result.get('explanation', ''),
                            'attempt': attempt + 1
                        }
                
                # JSONæå–å¤±è´¥ï¼Œå‡†å¤‡é‡è¯•
                if attempt < max_retries - 1:
                    logging.warning(f"JSONæå–å¤±è´¥ï¼Œç¬¬{attempt+1}æ¬¡é‡è¯•: {file_path}")
                    
                    # é‡æ–°æ„é€ æ›´ä¸¥æ ¼çš„æç¤ºè¯è¿›è¡Œé‡è¯•
                    retry_prompt = f"""
ä¸Šä¸€æ¬¡å›å¤æ ¼å¼ä¸æ­£ç¡®ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–å†…å®¹ï¼š

```json
{{
    "rewritten_code": "å®Œæ•´çš„Pythonä»£ç ï¼ˆåŒ…å«importè¯­å¥ï¼‰",
    "required_packages": ["package1", "package2"],
    "explanation": "é‡å†™è¯´æ˜"
}}
```

è¦æ±‚ï¼š
1. required_packageså¿…é¡»æ˜¯å‡†ç¡®çš„pipå®‰è£…åŒ…å
2. rewritten_codeå¿…é¡»æ˜¯å®Œæ•´å¯è¿è¡Œçš„Pythonä»£ç 
3. å¿…é¡»ä¸¥æ ¼éµå¾ªJSONæ ¼å¼

è¯·é‡å†™æ–‡ä»¶: {file_path}
"""
                    response = self._call_llm(retry_prompt)
                
            except json.JSONDecodeError as e:
                logging.warning(f"JSONè§£æå¤±è´¥: {e}, å°è¯•{attempt+1}/{max_retries}")
                continue
            except Exception as e:
                logging.warning(f"JSONæå–å¼‚å¸¸: {e}, å°è¯•{attempt+1}/{max_retries}")
                continue
        
        # æ‰€æœ‰é‡è¯•å¤±è´¥
        logging.error(f"JSONæå–å®Œå…¨å¤±è´¥: {file_path}")
        return {
            'success': False,
            'error': 'JSONæå–å¤±è´¥',
            'fallback_response': response,
            'attempts': max_retries
        }

    def _analyze_file_dependencies_fallback(self, original_content: str, rewritten_content: str) -> Set[str]:
        """fallbackæƒ…å†µä¸‹çš„æ–‡ä»¶ä¾èµ–åˆ†æï¼ˆä¿ç•™ä½œä¸ºå…œåº•æ–¹æ¡ˆï¼‰"""
        try:
            imports = set()
            
            # ç®€åŒ–çš„importæå–
            for match in re.finditer(r'^import\s+([\w.]+)', rewritten_content, re.MULTILINE):
                package = match.group(1).split('.')[0]
                imports.add(package)
            
            for match in re.finditer(r'^from\s+([\w.]+)\s+import', rewritten_content, re.MULTILINE):
                package = match.group(1).split('.')[0]
                imports.add(package)
            
            # åŸºç¡€çš„æ ‡å‡†åº“è¿‡æ»¤
            standard_libs = {
                'os', 'sys', 'logging', 'json', 'time', 'datetime', 'random',
                'math', 're', 'collections', 'itertools', 'functools', 'typing',
                'pathlib', 'subprocess', 'threading', 'multiprocessing', 'ast',
                'inspect', 'pickle', 'copy', 'csv', 'urllib', 'http', 'socket'
            }
            
            # åŸºç¡€çš„åŒ…åæ˜ å°„
            package_mapping = {
                'cv2': 'opencv-python',
                'PIL': 'Pillow', 
                'sklearn': 'scikit-learn',
                'psutil': 'psutil'
            }
            
            third_party = set()
            for pkg in imports:
                if pkg not in standard_libs:
                    actual_pkg = package_mapping.get(pkg, pkg)
                    third_party.add(actual_pkg)
            
            return third_party
            
        except Exception as e:
            logging.debug(f"fallbackä¾èµ–åˆ†æå¤±è´¥: {str(e)}")
            return set()
    
    def _extract_dependencies_from_config(self, file_path: str, filename: str):
        """ä»é…ç½®æ–‡ä»¶ä¸­æå–ä¾èµ–"""
        try:
            content = read_file_content(file_path)
            if not content:
                return
            
            if filename == 'requirements.txt':
                for line in content.split('\n'):
                    line = line.strip()
                    if line and not line.startswith('#'):
                        # æå–åŒ…åï¼ˆå»é™¤ç‰ˆæœ¬å·ï¼‰
                        pkg_name = re.split(r'[>=<!=]', line)[0].strip()
                        self.dependencies.add(pkg_name)
            
            elif filename == 'setup.py':
                # ä»setup.pyä¸­æå–install_requires
                install_requires_match = re.search(r'install_requires\s*=\s*\[(.*?)\]', content, re.DOTALL)
                if install_requires_match:
                    requirements = install_requires_match.group(1)
                    for req in re.findall(r'["\']([^"\']+)["\']', requirements):
                        pkg_name = re.split(r'[>=<!=]', req)[0].strip()
                        self.dependencies.add(pkg_name)
            
            # æ·»åŠ æ ‡å‡†ä¾èµ–
            self.dependencies.update(['logging', 'os', 'sys'])
            
        except Exception as e:
            logging.error(f"æå–ä¾èµ–å¤±è´¥ {file_path}: {str(e)}")
    
    def _generate_requirements_file(self, test_repo_path: str) -> str:
        """ç”Ÿæˆrequirements.txtæ–‡ä»¶"""
        try:
            requirements_path = os.path.join(test_repo_path, 'requirements.txt')
            
            # æ ‡å‡†åº“ä¸éœ€è¦å®‰è£…ï¼Œè¿‡æ»¤æ‰
            standard_libs = {
                'os', 'sys', 'logging', 'json', 'time', 'datetime', 'random', 
                'math', 're', 'collections', 'itertools', 'functools', 'typing'
            }
            
            installable_deps = [dep for dep in self.dependencies if dep not in standard_libs]
            
            # å†™å…¥requirementsæ–‡ä»¶
            with open(requirements_path, 'w', encoding='utf-8') as f:
                f.write("# Auto-generated requirements for sandbox testing\n")
                f.write("# æ²™ç®±æµ‹è¯•è‡ªåŠ¨ç”Ÿæˆçš„ä¾èµ–æ–‡ä»¶\n\n")
                for dep in sorted(installable_deps):
                    f.write(f"{dep}\n")
            
            logging.info(f"ç”Ÿæˆrequirementsæ–‡ä»¶: {requirements_path}")
            return requirements_path
            
        except Exception as e:
            logging.error(f"ç”Ÿæˆrequirementsæ–‡ä»¶å¤±è´¥: {str(e)}")
            return ""
    
    def _generate_project_summary(self, rewritten_files: List[Dict[str, Any]], structure_analysis: Dict[str, Any]) -> str:
        """ç”Ÿæˆé¡¹ç›®æ‘˜è¦"""
        try:
            python_files = [f for f in rewritten_files if f.get('type') == 'python']
            total_log_points = sum(f.get('log_points', 0) for f in python_files)
            rewritten_count = len([f for f in python_files if f.get('rewritten')])
            
            summary = f"""
ä»£ç é‡å†™å®Œæˆæ‘˜è¦ï¼š
- æ€»æ–‡ä»¶æ•°ï¼š{len(rewritten_files)}
- Pythonæ–‡ä»¶æ•°ï¼š{len(python_files)}
- æˆåŠŸé‡å†™ï¼š{rewritten_count}ä¸ªæ–‡ä»¶
- æ·»åŠ æ—¥å¿—ç‚¹ï¼š{total_log_points}ä¸ª
- ç”Ÿæˆä¾èµ–ï¼š{len(self.dependencies)}ä¸ª
- æ¨èå…¥å£ï¼š{structure_analysis.get('recommended_entry', 'æœªç¡®å®š')}
"""
            return summary.strip()
            
        except Exception as e:
            logging.error(f"ç”Ÿæˆé¡¹ç›®æ‘˜è¦å¤±è´¥: {str(e)}")
            return "æ‘˜è¦ç”Ÿæˆå¤±è´¥"

    def _aggregate_dependencies_from_results(self, results: List[Dict[str, Any]]):
        """ä»é‡å†™ç»“æœä¸­èšåˆæ‰€æœ‰æ–‡ä»¶çš„ä¾èµ–åŒ…"""
        try:
            total_deps_count = 0
            for result in results:
                if 'dependencies' in result and result['dependencies']:
                    file_deps = set(result['dependencies'])
                    self.dependencies.update(file_deps)
                    total_deps_count += len(file_deps)
                    
                    # è®°å½•è¯¦ç»†çš„ä¾èµ–ä¿¡æ¯ç”¨äºè°ƒè¯•
                    file_path = result.get('original_path', 'unknown')
                    logging.debug(f"æ–‡ä»¶ {file_path} çš„ä¾èµ–: {file_deps}")
            
            logging.info(f"ä¾èµ–èšåˆå®Œæˆ: æ”¶é›†äº† {total_deps_count} ä¸ªæ–‡ä»¶çº§ä¾èµ–ï¼Œå»é‡åæ€»è®¡ {len(self.dependencies)} ä¸ªå”¯ä¸€åŒ…")
            
        except Exception as e:
            logging.error(f"èšåˆä¾èµ–å¤±è´¥: {str(e)}")
    
 